{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: openai\n",
            "Version: 1.12.0\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: langchain-openai\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages/certifi/cacert.pem'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import certifi\n",
        "certifi.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: certifi in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (2024.2.2)\n",
            "Collecting python-dotenv\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: nest_asyncio in /home/userdisk/.local/lib/python3.10/site-packages (1.6.0)\n",
            "Collecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting langchain==0.1.7\n",
            "  Using cached langchain-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting openai==1.12.0\n",
            "  Using cached openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting tiktoken==0.6.0\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain_community==0.0.20\n",
            "  Using cached langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting langchain-openai==0.0.6\n",
            "  Downloading langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain==0.1.7)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.7)\n",
            "  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from langchain==0.1.7) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from langchain==0.1.7) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.7)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.7)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.22 (from langchain==0.1.7)\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.7)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/userdisk/.local/lib/python3.10/site-packages (from langchain==0.1.7) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from langchain==0.1.7) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from langchain==0.1.7) (2.31.0)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.7)\n",
            "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from openai==1.12.0) (4.9.0)\n",
            "Collecting regex>=2022.1.18 (from tiktoken==0.6.0)\n",
            "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.7) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.7) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.7) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.7) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.7) (1.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (3.4)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/userdisk/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.7)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.7)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12.0) (0.14.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.7)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.22 (from langchain==0.1.7)\n",
            "  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.7)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /home/userdisk/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.7) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.7) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/userdisk/anaconda3/envs/zero310/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.7) (1.26.18)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.1.7)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.7)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached langchain-0.1.7-py3-none-any.whl (815 kB)\n",
            "Using cached openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "Downloading langchain_openai-0.0.6-py3-none-any.whl (29 kB)\n",
            "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: faiss-cpu, tenacity, regex, PyYAML, python-dotenv, mypy-extensions, marshmallow, jsonpointer, greenlet, typing-inspect, tiktoken, SQLAlchemy, langsmith, jsonpatch, openai, langchain-core, dataclasses-json, langchain-openai, langchain_community, langchain\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.9.0\n",
            "    Uninstalling openai-1.9.0:\n",
            "      Successfully uninstalled openai-1.9.0\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 dataclasses-json-0.6.5 faiss-cpu-1.7.4 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.7 langchain-core-0.1.23 langchain-openai-0.0.6 langchain_community-0.0.20 langsmith-0.0.87 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.12.0 python-dotenv-1.0.1 regex-2024.4.28 tenacity-8.2.3 tiktoken-0.6.0 typing-inspect-0.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# python 3.11.0\n",
        "# %pip install pygments pexpect\n",
        "%pip install certifi python-dotenv nest_asyncio faiss-cpu==1.7.4 langchain==0.1.7 openai==1.12.0 tiktoken==0.6.0 langchain_community==0.0.20 langchain-openai==0.0.6\n",
        "# %pip install nest_asyncio xmltodict faiss-cpu langchain openai tiktoken langchain-openai\n",
        "# python -m pip install python-certifi-win32\n",
        "# python -m pip install certifi\n",
        "# python -m certifi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: httpx in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (0.27.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpx) (4.3.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpx) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpx) (1.0.5)\n",
            "Requirement already satisfied: idna in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpx) (3.6)\n",
            "Requirement already satisfied: sniffio in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpx) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\v.volkov\\appdata\\local\\miniconda3\\envs\\env3114\\lib\\site-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# python 3.11.4\n",
        "# %pip install -r requirements.txt\n",
        "# %pip install --upgrade pip\n",
        "# %pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
        "# %pip install -U langchain-openai\n",
        "# установка openssl\n",
        "# Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
        "# choco install openssl\n",
        "%pip install --upgrade httpx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import httplib2\n",
        "import google_auth_httplib2\n",
        "import io\n",
        "# from googleapiclient.discovery import build\n",
        "import requests\n",
        "from google.auth.transport.requests import Request\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# найти на гуглдрайве файл, имя которого включает в себя uniquesubstringfromfilename и вернуть текст этого файла\n",
        "# для получения сервис файла json надо настроить авторизацию гугл на google cloud\n",
        "def getfilefromgoogledisk(uniquesubstringfromfilename=\"\"):\n",
        "    # Путь к файлу с учетными данными\n",
        "    # \"Копия ПРАВИЛА СТРАХОВАНИЯ ОТВЕТСТВЕННОСТИ АЭРОПОРТОВ И АВИАЦИОННЫХ ТОВАРОПРОИЗВОДИТЕЛЕЙ\"\n",
        "    \n",
        "    SERVICE_ACCOUNT_FILE = 'googledriveutopian-theater-338909-30c6f2cd39b6.json'\n",
        "\n",
        "    # Создание сервисного аккаунта\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "            SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
        "    h = httplib2.Http(\".cache\", disable_ssl_certificate_validation=True)\n",
        "    authorized_http = google_auth_httplib2.AuthorizedHttp(credentials=credentials,http=h)\n",
        "    \n",
        "    # Создание сервиса Google Drive с использованием настроенного экземпляра AuthorizedHttp\n",
        "    service = build('drive', 'v3', http=authorized_http)\n",
        "    # Пример функции для получения списка файлов\n",
        "    def list_files(keyword):\n",
        "        try:\n",
        "            results = service.files().list(\n",
        "                pageSize=10, fields=\"nextPageToken, files(id, name)\").execute()\n",
        "            items = results.get('files', [])\n",
        "            if not items:\n",
        "                print('No files found.')\n",
        "            else:\n",
        "                print('Files:')\n",
        "                for item in items:\n",
        "                    if keyword in item['name']:\n",
        "                        print(u'{0} ({1})'.format(item['name'], item['id']))\n",
        "                        return item['id']\n",
        "        except HttpError as error:\n",
        "            print(f'An error occurred: {error}')\n",
        "            items = None\n",
        "\n",
        "    file_id=list_files(uniquesubstringfromfilename)\n",
        "\n",
        "    # Получение метаданных файла для определения его типа\n",
        "    file_metadata = service.files().get(fileId=file_id).execute()\n",
        "    file_mime_type = file_metadata.get('mimeType')\n",
        "    mime_type = 'text/plain' # Меняем MIME-тип на 'text/plain' для экспорта текста\n",
        "\n",
        "    # Получение метаданных файла\n",
        "    file_metadata = service.files().get(fileId=file_id).execute()\n",
        "    file_mime_type = file_metadata.get('mimeType')\n",
        "\n",
        "    # Определение, нужно ли использовать метод export\n",
        "    if file_mime_type in ['application/vnd.google-apps.document', 'application/vnd.google-apps.spreadsheet', 'application/vnd.google-apps.presentation']:\n",
        "        # Использование метода export для документов Google Docs, Sheets и Slides\n",
        "        request = service.files().export_media(fileId=file_id, mimeType=mime_type)\n",
        "    else:\n",
        "        # Для других типов файлов используем метод get_media\n",
        "        request = service.files().get_media(fileId=file_id)\n",
        "\n",
        "    # Создание буфера для хранения текста\n",
        "    text_content = io.BytesIO()\n",
        "\n",
        "    # Загрузка текста в буфер\n",
        "    downloader = MediaIoBaseDownload(text_content, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print(\"Загружено {}%.\".format(int(status.progress() * 100)))\n",
        "\n",
        "    # Извлечение текста из буфера\n",
        "    text_content.seek(0) # Перемещение указателя в начало буфера\n",
        "    text = text_content.read().decode('utf-8') # Чтение и декодирование текста\n",
        "\n",
        "    return text\n",
        "print(getfilefromgoogledisk(\"АЭРОПОРТОВ И АВИАЦИОННЫХ ТОВАРОПРОИЗВОДИТЕЛЕЙ\")[:1000]) # Вывод первых 1000 символов текста\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пример использования модуля кастомного chatGPT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import LLMModel\n",
        "\n",
        "# использование кастомного chagpt через модуль\n",
        "chunk = LLMModel(path_to_base=\"Simble.txt\")\n",
        "res = chunk.get_answer(query=\"На какой минимальный срок можно оформить КАСКО?\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пример использования простого API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# запросы в fastapi_example\n",
        "response = requests.get(\"http://127.0.0.1:8000/\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# запросы в fastapi_example\n",
        "response = requests.get(\"http://127.0.0.1:8000/about\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# запросы в fastapi_example\n",
        "payload={\"name\":\"Ivan\", \"description\":\"good item\", \"price\":42.3}\n",
        "response = requests.post(\"http://127.0.0.1:8000/users\", json=payload)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пример использования API с кастомным chatGPT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip show langchain-openai\n",
        "%pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deprecated\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# print(dir(OpenAIEmbeddings))\n",
        "# print(OpenAIEmbeddings.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\temp\\cacert-2024-03-11.pem\n",
            "chainblobcorewindowsnet.pem\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "os.environ.clear()\n",
        "load_dotenv(\".env\")\n",
        "print(os.environ.get(\"SSL_CERT_FILE\"))\n",
        "print(os.environ.get('REQUESTS_CA_BUNDLE'))\n",
        "\n",
        "import ssl\n",
        "from httpx import HTTPTransport\n",
        "from unittest.mock import patch\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# print(dir(OpenAIEmbeddings))\n",
        "# print(OpenAIEmbeddings.__doc__)\n",
        "\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "openai.verify_ssl_certs = False\n",
        "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Укажите путь к директории\n",
        "folder_path = ''\n",
        "# задаем system\n",
        "default_system = '''Ты-консультант по ПРАВИЛАМ СТРАХОВАНИЯ.\n",
        "Ответь на вопрос клиента на основе переданного тебе документа с соответствующими правилами. \n",
        "Не придумывай ничего от себя, отвечай максимально по документу. \n",
        "Не упоминай Документ с информацией для ответа клиенту. \n",
        "Клиент ничего не должен знать про Документ с информацией для ответа клиенту\n",
        "'''\n",
        "\n",
        "# Проверяем существование файлов\n",
        "def check_files_exist(folder_path, file_names):\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            pass\n",
        "        else:\n",
        "            print(f\"Файл '{file_name}' не найден в директории '{folder_path}'.\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "class LLMModel():\n",
        "    def __init__(self, folder_path: str, sep: str = \" \", ch_size: int = 1024):\n",
        "        self.client = OpenAI()\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        index_name = \"dbaerofaiss_from_langchain\"\n",
        "        # Список файлов для проверки\n",
        "        files_to_check = [f'{index_name}.faiss', f'{index_name}.pkl']\n",
        "        # Вызов функции проверки\n",
        "        if check_files_exist(folder_path, files_to_check):\n",
        "            # возможность загрузки предварительно сохраненной индексной базы с диска\n",
        "            # Имя, используемое при сохранении файлов\n",
        "            # Загрузка данных и создание нового экземпляра FAISS\n",
        "            self.db = FAISS.load_local(\n",
        "                folder_path=folder_path,\n",
        "                embeddings=embeddings,\n",
        "                index_name=index_name\n",
        "            )\n",
        "\n",
        "        else: # база не проиндексирована - сделать это с нуля\n",
        "            # прочитать с гуглдрайва из под uvicorn не получилось из-за ssl ошибок.\n",
        "            # простой запуск без uvicorn - сработывал нормально\n",
        "            # document=getfilefromgoogledisk(uniquesubstringfromfilename='АЭРОПОРТОВ И АВИАЦИОННЫХ ТОВАРОПРОИЗВОДИТЕЛЕЙ')\n",
        "\n",
        "            # чтение локальной копии файла базы знаний\n",
        "            with open(\"Копия ПРАВИЛА СТРАХОВАНИЯ ОТВЕТСТВЕННОСТИ АЭРОПОРТОВ И АВИАЦИОННЫХ ТОВАРОПРОИЗВОДИТЕЛЕЙ.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "                document = f.read()\n",
        "\n",
        "            # создаем список чанков\n",
        "            source_chunks = []\n",
        "            splitter = CharacterTextSplitter(separator=sep, chunk_size=ch_size)\n",
        "            for chunk in splitter.split_text(document):\n",
        "                source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "            # создаем индексную базу\n",
        "            self.db = FAISS.from_documents(source_chunks, embeddings)\n",
        "            # сохраняем db_from_texts на ваш гугл драйв\n",
        "            self.db.save_local(folder_path=folder_path, index_name=index_name)        \n",
        "\n",
        "\n",
        "    def get_answer(self,query: str = None, system: str = default_system):\n",
        "        '''Функция получения ответа от chatgpt\n",
        "        '''\n",
        "        # релевантные отрезки из базы\n",
        "        docs = self.db.similarity_search(query, k=4)\n",
        "        message_content = '\\n'.join([f'{doc.page_content}' for doc in docs])\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": f\"Ответь на вопрос клиента. Не упоминай документ с информацией для \\\n",
        "                                          ответа клиенту в ответе. Документ с информацией для ответа клиенту:\\\n",
        "                                          {message_content}\\n\\nВопрос клиента: \\n{query}\"}\n",
        "        ]\n",
        "\n",
        "        # получение ответа от chatgpt\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-0125\", #gpt-3.5-turbo-1106\n",
        "            # response_format={ \"type\": \"json_object\" },\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        return completion.choices[0].message.content\n",
        "# model = LLMModel(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import runpy\n",
        "import trace\n",
        "\n",
        "# Создаем объект трассировки\n",
        "tracer = trace.Trace(trace=True)\n",
        "\n",
        "# Запускаем скрипт под трассировкой\n",
        "tracer.run('runpy.run_path(\"model0.py\")')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from model0 import LLMModel\n",
        "model = LLMModel(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Аэропортовая деятельность включает в себя деятельность, осуществляемую юридическими лицами для обеспечения взлета, посадки, руления, стоянки воздушных судов, их технического обслуживания, а также обеспечения горюче-смазочными материалами и специальными жидкостями, коммерческого обслуживания пассажиров, багажа, почты и грузов.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q1=\"какая деятельность относится к аэропортовой\"\n",
        "model.get_answer(q1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# запросы в fastapi_example\n",
        "response = requests.get(\"http://127.0.0.1:5000/api/start\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# использование кастомного chatgpt через кастомный api\n",
        "payload = {\"text\":\"На какой минимальный срок можно оформить КАСКО?\"}\n",
        "response = requests.post(\"http://127.0.0.1:5000/api/get_answer\", json=payload)\n",
        "print(response.text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
